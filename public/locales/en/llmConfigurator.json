{
    "choose_llm_model": "Choose LLM Model",
    "choose_llm_version": "Choose LLM Version",
    "tokens_generated": "Tokens generated on each LLM output",
    "tokens_helper_text": "Increasing tokens enables longer responses to be queued before sending to speech generation but increases latency.",
    "temperature": "Temperature",
    "temperature_helper_text": "Increasing temperature enables heightened creativity but increases the chance of deviation from the prompt.",
    "saving": "Saving...",
    "save": "Save",
    "save_success": "Agent configuration saved successfully"
  }
  